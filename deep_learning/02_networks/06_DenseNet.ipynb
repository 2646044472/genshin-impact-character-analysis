{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8ae1c6-92b8-4170-bb53-77a381d1b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae59603-e4fb-4b2a-b575-b66fecd9c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义dense block中的dense layer\n",
    "class _DenseLayer(nn.Module):\n",
    "    # 构造函数，接收输入通道数num_input_features、输出通道数growth_rate、卷积层的输出比例bn_size\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size):\n",
    "        super().__init__()\n",
    "        # 定义第一个卷积层，包括BN层、ReLU激活函数和1x1卷积层\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_input_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
    "        )\n",
    "        # 定义第二个卷积层，包括BN层、ReLU激活函数和3x3卷积层\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(bn_size * growth_rate),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    # 定义前向传播函数\n",
    "    def forward(self, x):\n",
    "        # BN+ReLU+1x1卷积\n",
    "        out = self.conv1(x)\n",
    "        # BN+ReLU+3x3卷积\n",
    "        out = self.conv2(out)\n",
    "        return torch.cat([x, out], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988e3c0f-f7d8-44f8-af88-a9f519f39367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义dense block\n",
    "class _DenseBlock(nn.Module):\n",
    "    # 构造函数，包含密集连接层的数量num_layers、输入通道数num_input_features、输出通道数growth_rate、卷积层的\n",
    "    def __init__(self, num_layers, num_input_features, growth_rate, bn_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # 保存密集连接层的列表\n",
    "        layers = []\n",
    "        # 构建num_layers个密集连接层\n",
    "        for i in range(num_layers):\n",
    "            # 构建一个密集连接层，其中输入通道数为num_input_features + i * growth_rate逐层递增\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size)\n",
    "            # 将构建的密集连接层添加到列表中保存\n",
    "            layers.append(layer)\n",
    "        # 将所有密集连接层封装到Sequential中保存为block\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    # 定义前向传播函数\n",
    "    def forward(self, x):\n",
    "        # 经过当前block输出即可\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0699ab99-f17f-4628-97bf-9a5ee16bd90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义dense block之间的transition layer\n",
    "class _Transition(nn.Module):\n",
    "    # 构造函数，输入通道数num_input_features, 输出通道数num_output_features\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super().__init__()\n",
    "        # 定义一个转换层，用于降维和调整特征图的size. 包含BN+ReLU+1x1卷积+平均池化层\n",
    "        self.trans = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_input_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    # 定义前向传播函数\n",
    "    def forward(self, x):\n",
    "        # 经过转换层后输出即可\n",
    "        return self.trans(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ee162d-f14c-4a85-ab39-485ceeda1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义DenseNet的网络结构\n",
    "class DenseNet(nn.Module):\n",
    "    # 构造函数，包含dense block的数量block_config, 输入通道数num_init_features, 输出通道数growth_rate,\n",
    "    #          卷积层的缩放比例bn_size和类别数num_classes\n",
    "    def __init__(self, block_config, num_init_features=64, growth_rate=32, bn_size=4, num_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # 第一部分：7x7卷积+BN+ReLU+最大池化层\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(num_init_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # 下面依次定义dense block和transition layer, 对应第二部分和第三部分\n",
    "        num_features = num_init_features # 记录通道数\n",
    "        layers = [] # 网络结构保存列表\n",
    "        # 遍历每层dense block的数量列表\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            # 创建dense block, 其中包含num_layers个dense layer\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                growth_rate=growth_rate, bn_size=bn_size)\n",
    "            layers.append(block)\n",
    "            num_features = num_features + num_layers * growth_rate # 更新特征图维度\n",
    "            # 如果不是最后一个dense block，则添加一个transition layer，特征图维度除以2\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                layers.append(trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # 添加一个BN层\n",
    "        layers.append(nn.BatchNorm2d(num_features))\n",
    "        # 调用nn.Sequential完成第二部分和第三部分\n",
    "        self.denseblock = nn.Sequential(*layers)\n",
    "\n",
    "        # 第四部分：全连接层\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    # 定义前向传播函数\n",
    "    def forward(self, x):\n",
    "        # 第一部分\n",
    "        features = self.features(x)\n",
    "        # 第二、三部分\n",
    "        features = self.denseblock(features)\n",
    "        # ReLU\n",
    "        out = F.relu(features, inplace=True)\n",
    "        # 第四部分：平均池化+全连接层\n",
    "        out = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        # 输出\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4550165-d5d6-4640-941b-8d92a7766aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 封装函数对应4个模型，num_classes表示类别数\n",
    "# 其中数值与论文中的数值一致\n",
    "def densenet121(num_classes=1000):\n",
    "    return DenseNet(block_config=(6, 12, 24, 16), num_init_features=64,\n",
    "                    growth_rate=32, num_classes=num_classes)\n",
    "\n",
    "def densenet161(num_classes=1000):\n",
    "    return DenseNet(block_config=(6, 12, 36, 24), num_init_features=96,\n",
    "                    growth_rate=48, num_classes=num_classes)\n",
    "\n",
    "def densenet169(num_classes=1000):\n",
    "    return DenseNet(block_config=(6, 12, 32, 32), num_init_features=64,\n",
    "                    growth_rate=32, num_classes=num_classes)\n",
    "\n",
    "def densenet201(num_classes=1000):\n",
    "    return DenseNet(block_config=(6, 12, 48, 32), num_init_features=64,\n",
    "                    growth_rate=32, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b892dde-a7d5-4aaf-b31a-add8dbd7c9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "DenseNet                                           [1, 1000]                 --\n",
       "├─Sequential: 1-1                                  [1, 64, 56, 56]           --\n",
       "│    └─Conv2d: 2-1                                 [1, 64, 112, 112]         9,408\n",
       "│    └─BatchNorm2d: 2-2                            [1, 64, 112, 112]         128\n",
       "│    └─ReLU: 2-3                                   [1, 64, 112, 112]         --\n",
       "│    └─MaxPool2d: 2-4                              [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-2                                  [1, 1024, 7, 7]           --\n",
       "│    └─_DenseBlock: 2-5                            [1, 256, 56, 56]          --\n",
       "│    │    └─Sequential: 3-1                        [1, 256, 56, 56]          335,040\n",
       "│    └─_Transition: 2-6                            [1, 128, 28, 28]          --\n",
       "│    │    └─Sequential: 3-2                        [1, 128, 28, 28]          33,280\n",
       "│    └─_DenseBlock: 2-7                            [1, 512, 28, 28]          --\n",
       "│    │    └─Sequential: 3-3                        [1, 512, 28, 28]          919,680\n",
       "│    └─_Transition: 2-8                            [1, 256, 14, 14]          --\n",
       "│    │    └─Sequential: 3-4                        [1, 256, 14, 14]          132,096\n",
       "│    └─_DenseBlock: 2-9                            [1, 1024, 14, 14]         --\n",
       "│    │    └─Sequential: 3-5                        [1, 1024, 14, 14]         2,837,760\n",
       "│    └─_Transition: 2-10                           [1, 512, 7, 7]            --\n",
       "│    │    └─Sequential: 3-6                        [1, 512, 7, 7]            526,336\n",
       "│    └─_DenseBlock: 2-11                           [1, 1024, 7, 7]           --\n",
       "│    │    └─Sequential: 3-7                        [1, 1024, 7, 7]           2,158,080\n",
       "│    └─BatchNorm2d: 2-12                           [1, 1024, 7, 7]           2,048\n",
       "├─Linear: 1-3                                      [1, 1000]                 1,025,000\n",
       "====================================================================================================\n",
       "Total params: 7,978,856\n",
       "Trainable params: 7,978,856\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.83\n",
       "====================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 180.54\n",
       "Params size (MB): 31.92\n",
       "Estimated Total Size (MB): 213.06\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模型结构及参数量，input_size表示示例输入数据的维度信息\n",
    "summary(densenet121(), input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44e26ecb-91d1-41ce-b539-6dc4c9b457e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")# 设置epoch数并开始训练\n",
    "model = densenet121(num_classes=102).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c60e8b-b0ad-4a39-978b-5fba6a8484b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 设置训练集的数据变换，进行数据增强\n",
    "trainform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(30),  # 随机旋转 -30度到30度之间\n",
    "    transforms.RandomResizedCrop((224, 224)),  # 随机裁剪调整大小进行resize\n",
    "    transforms.RandomHorizontalFlip(p = 0.5),  # 随机水平翻转\n",
    "    transforms.RandomVerticalFlip(p = 0.5),  # 随机垂直翻转\n",
    "    transforms.ToTensor(),  # 将图片转换为张量\n",
    "    # 对三通道数据进行归一化(均值，标准差)，数值是从ImageNet数据集上的百万张图片中随机抽样计算得到\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 设置测试集的数据变换，不进行数据增强，仅使用resize和归一化\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # resize\n",
    "    transforms.ToTensor(),  # 将数据转换为张量\n",
    "    # 对三通道数据进行归一化(均值，标准差)，数值是从ImageNet数据集上的百万张图片中随机抽样计算得到\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 加载训练数据，需要特别注意的是Flowers102数据集，test集的数据量较多些，所以这里使用\"test\"作为训练集\n",
    "train_dataset = datasets.Flowers102(root='../data/flowers102', split=\"test\",\n",
    "                                    download=True, transform=trainform_train)\n",
    "# 实例化训练数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=10, pin_memory=True)\n",
    "\n",
    "# 加载测试数据，使用\"train\"作为测试集\n",
    "test_dataset = datasets.Flowers102(root='../data/flowers102', split=\"train\",\n",
    "                                   download=True, transform=transform_test)\n",
    "# 实例化测试数据加载器\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=10, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f502b9-c5e3-4ec5-bd5d-75c5fc12f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置epoch数并开始训练\n",
    "num_epochs = 200  # 设置epoch数\n",
    "loss_history = []  # 创建损失历史记录列表\n",
    "acc_history = []   # 创建准确率历史记录列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29f888f0-d70f-4b4a-9c59-02a219b6c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2575914d-a2e7-4819-945c-e54cf09cc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ed748-8dca-4abd-ab49-994cbdeeedbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utopia/anaconda3/envs/nn/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2.557447483675432 Acc: 0.11862745098039215                       \n",
      "Epoch: 5 Loss: 2.349557495356514 Acc: 0.2784313725490196                        \n",
      "Epoch: 10 Loss: 2.260569950769129 Acc: 0.3656862745098039                       \n",
      "Epoch: 15 Loss: 2.196075421801445 Acc: 0.4019607843137255                       \n",
      "Epoch: 20 Loss: 2.1367064594524328 Acc: 0.47941176470588237                     \n",
      "Epoch: 25 Loss: 2.065131694126065 Acc: 0.5294117647058824                       \n",
      "Epoch: 30 Loss: 2.0207972453354834 Acc: 0.5509803921568628                      \n",
      "Epoch: 35 Loss: 1.9497999669499533 Acc: 0.5892156862745098                      \n",
      " 20%|████████                                | 40/200 [24:59<1:38:12, 36.83s/it]"
     ]
    }
   ],
   "source": [
    "# tqdm用于显示进度条并评估在每时间开销\n",
    "for epoch in tqdm(range(num_epochs), file=sys.stdout):\n",
    "    # 记录损失和预测正确数\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # 批量训练\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        # 将数据转移到指定计算资源设备上\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 预测、损失函数、反向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 记录训练集loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        '''\n",
    "        # 测试模型，不计算梯度\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                # 将数据转移到指定计算资源设备上\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # 预测\n",
    "                outputs = model(inputs)\n",
    "                # 记录测试集预测正确数\n",
    "                total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        \n",
    "        # 记录训练集损失和测试集准确率\n",
    "        loss_history.append(np.log10(total_loss))  # 将损失加入损失历史记录列表，由于数值有时较大，这里取对数\n",
    "        acc_history.append(total_correct / len(test_dataset)) # 将准确率加入准确率历史记录列表\n",
    "        '''\n",
    "        \n",
    "    # 打印中间值\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                # 将数据转移到指定计算资源设备上\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # 预测\n",
    "                outputs = model(inputs)\n",
    "                # 记录测试集预测正确数\n",
    "                total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        loss_history.append(np.log10(total_loss))  # 将损失加入损失历史记录列表，由于数值有时较大，这里取对数\n",
    "        acc_history.append(total_correct / len(test_dataset)) # 将准确率加入准确率历史记录列表\n",
    "        tqdm.write(\"Epoch: {0} Loss: {1} Acc: {2}\".format(epoch, loss_history[-1], acc_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fa2b9-081c-4a13-adf5-bae4dd555eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Matplotlib绘制损失和准确率的曲线图\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_history, label='loss')\n",
    "plt.plot(acc_history, label='accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy:\",acc_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f835ef-9842-4509-b09e-2bfc0fd92ee9",
   "metadata": {},
   "source": [
    "DenseNet为什么会比ResNet在训练时明显占用更多内存"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
